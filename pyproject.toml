[build-system]
requires = ["setuptools", "setuptools-scm"]
build-backend = "setuptools.build_meta"

[project]
name = "cible"
dynamic = ["version"]
description = "Perfectionnement du LLM"
readme = "README.md"
requires-python = ">=3.9"
license = {file = "LICENSE"}
keywords = ["ai", "llm", "ci", "dÃ©v",]
authors = [
    {email = "cibleinfo@llm.ci"},
    {name = "Cible AI team"},
]
classifiers = [
    "Programming Language :: Python",
]

[tool.setuptools.dynamic]
version = {attr = "cible.models._utils.__version__"}

[tool.setuptools]
include-package-data = false

[tool.setuptools.packages.find]
exclude = ["images*"]

[project.optional-dependencies]
huggingface = [
    "packaging",
    "tyro",
    "transformers>=4.43.2",
    "datasets>=2.16.0",
    "sentencepiece>=0.2.0",
    "tqdm",
    "psutil",
    "wheel>=0.42.0",
    "numpy",
    "accelerate>=0.26.1",
    "trl>=0.7.9,<0.9.0",
    "peft>=0.7.1,!=0.11.0",
    "protobuf<4.0.0",
    "huggingface_hub",
    "hf_transfer",
]
cu118only = [
    "xformers==0.0.22.post7",
]
cu121only = [
    "xformers==0.0.22.post7",
]
cu118onlytorch211 = [
    "xformers==0.0.23",
]
cu121onlytorch211 = [
    "xformers==0.0.23",
]
cu118onlytorch212 = [
    "xformers==0.0.23.post1",
]
cu121onlytorch212 = [
    "xformers==0.0.23.post1",
]
cu118onlytorch220 = [
    "xformers==0.0.24",
]
cu121onlytorch220 = [
    "xformers==0.0.24",
]
cu118onlytorch230 = [
    "xformers==0.0.26.post1",
]
cu121onlytorch230 = [
    "xformers==0.0.26.post1",
]

cu118 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118only]",
]
cu121 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121only]",
]
cu118-torch211 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118onlytorch211]",
]
cu121-torch211 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch211]",
]
cu118-torch212 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118onlytorch212]",
]
cu121-torch212 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch212]",
]
cu118-torch220 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118onlytorch220]",
]
cu121-torch220 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch220]",
]
cu118-torch230 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118onlytorch230]",
]
cu121-torch230 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch230]",
]
kaggle = [
    "cible[huggingface]",
]
kaggle-new = [
    "cible[huggingface]",
    "bitsandbytes",
]
conda = [
    "cible[huggingface]",
]
colab-torch211 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch211]",
]
colab-ampere-torch211 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch211]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
colab-torch220 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch220]",
]
colab-ampere-torch220 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch220]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
colab-new = [
    "packaging",
    "tyro",
    "transformers>=4.43.2",
    "datasets>=2.16.0",
    "sentencepiece>=0.2.0",
    "tqdm",
    "psutil",
    "wheel>=0.42.0",
    "numpy",
    "protobuf<4.0.0",
    "huggingface_hub",
    "hf_transfer",
]
colab-no-deps = [
    "accelerate>=0.26.1",
    "trl>=0.7.9",
    "peft>=0.7.1",
    "xformers<0.0.27",
    "bitsandbytes",
    "protobuf<4.0.0",
]
colab = [
    "cible[cu121]",
]
colab-ampere = [
    "cible[colab-ampere-torch220]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
cu118-ampere = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118only]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
cu121-ampere = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121only]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
cu118-ampere-torch211 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118onlytorch211]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
cu121-ampere-torch211 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch211]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
cu118-ampere-torch220 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118onlytorch220]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
cu121-ampere-torch220 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch220]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
cu118-ampere-torch230 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu118onlytorch230]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]
cu121-ampere-torch230 = [
    "cible[huggingface]",
    "bitsandbytes",
    "cible[cu121onlytorch230]",
    "packaging",
    "ninja",
    "flash-attn>=2.6.3",
]

[project.urls]
homepage = "http://cible.llm.ci"
documentation = "https://github.com/CibleLM/cible"
repository = "https://github.com/CibleLM/cible"
